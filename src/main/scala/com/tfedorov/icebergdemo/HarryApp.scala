/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.tfedorov.icebergdemo

import org.apache.spark.sql.SparkSession

object HarryApp {
  def main(args: Array[String]): Unit = {
    println("Start" + this.getClass.getName)
    runHarry()
    println("End" + this.getClass.getName)
  }

  def runHarry(): Unit = {

    //  val ICEBERG_TEST_PATH = "src/test/resources/iceberg/"
    val WAREHOUSE_DIR_PATH: String = "src/main/resources/iceberg/"
    val SPARK_CATALOG_PATH: String = WAREHOUSE_DIR_PATH + "spark_catalog/"

    lazy val spark: SparkSession = SparkSession
      .builder()
      .config("spark.driver.host", "localhost")
      .config("spark.sql.warehouse.dir", WAREHOUSE_DIR_PATH)
      .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
      .config("spark.sql.catalog.spark_catalog.type", "hadoop")
      .config("spark.sql.catalog.spark_catalog.warehouse", SPARK_CATALOG_PATH)
      .config("spark.sql.catalog.iceberg_test", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      // Iceberg configs
      .master("local[*]")
      .getOrCreate()

    val inputDF = spark.read
      .option("inferSchema", "true")
      .option("header", "true")
      .csv("src/main/resources/data/lightning.csv")
    inputDF.show()

    spark.sql("""CREATE NAMESPACE IF NOT EXISTS harry_ns""")
    spark.sql(
      """CREATE TABLE IF NOT EXISTS harry_ns.integrated_table (key string, value string)
        USING iceberg PARTITIONED BY (key)"""
    )
    spark.sql("SELECT * FROM harry_ns.integrated_table").show
  }
}
