/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.tfedorov.icebergdemo

import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.{DataFrame, SparkSession}

object InitAndLoadDataAPP {

  val WAREHOUSE_DIR_PATH: String = "src/main/resources/output_iceberg"
//  val SPARK_CATALOG_PATH: String = WAREHOUSE_DIR_PATH + "spark_catalog/"
  val HARRY_CATALOG_PATH: String = WAREHOUSE_DIR_PATH + "/catalog/harry_ns/"

  def main(args: Array[String]): Unit = {
    println("Start" + this.getClass.getName)
    createTableAndFill()
    upsertData()
    optimize()
    println("End" + this.getClass.getName)
  }

  private def createTableAndFill(): Unit = {

    val spark: SparkSession = SparkSession
      .builder()
      .master("local[*]")
      .config("spark.driver.host", "localhost")
      // Iceberg configs
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .config("spark.sql.catalog.harry_ns", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.harry_ns.type", "hadoop")
      .config("spark.sql.catalog.harry_ns.warehouse", HARRY_CATALOG_PATH)
      .config("spark.sql.warehouse.dir", WAREHOUSE_DIR_PATH)
      .getOrCreate()

    val inputDF: DataFrame = spark.read
      .option("delimiter", ";")
      .option("inferSchema", "true")
      .option("header", "true")
      .csv("src/main/resources/input_data/Characters.csv")

    inputDF.printSchema()
    inputDF.createGlobalTempView("input_data")

    val createTableSQL = """
     CREATE TABLE IF NOT EXISTS harry_ns.input_table (Id  string,
          Name  string,
          Gender  string,
          Job  string,
          House  string,
          Wand  string,
          Patronus  string,
          Species  string,
          Blood_status  string,
          Hair_colour  string,
          Eye_colour  string,
          Loyalty  string,
          Skills  string,
          Birth  string,
          Death  string
          )
        USING iceberg PARTITIONED BY (Gender)"""
    spark.sql(createTableSQL)

    val insertSQL =
      """
     INSERT INTO harry_ns.input_table
        SELECT Id,Name,Gender,Job,House,Wand,Patronus,Species,Blood_status,Hair_colour,Eye_colour,Loyalty,Skills,Birth,Death 
      FROM global_temp.input_data
      """
    spark.sql(insertSQL)
//    spark.sql("SELECT * FROM harry_ns.input_table").show
  }

  def upsertData(): Unit = {
    val spark: SparkSession = SparkSession
      .builder()
      .master("local[*]")
      .config("spark.driver.host", "localhost")
      // Iceberg configs
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .config("spark.sql.catalog.harry_ns", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.harry_ns.type", "hadoop")
      .config("spark.sql.catalog.harry_ns.warehouse", HARRY_CATALOG_PATH)
      .config("spark.sql.warehouse.dir", WAREHOUSE_DIR_PATH)
      .getOrCreate()

    val selectedRowDF: DataFrame = spark.sql("SELECT * FROM harry_ns.input_table WHERE id = 1")
    val data2Upsert = selectedRowDF
      .withColumn("id", lit("777"))
      .union(selectedRowDF.withColumn("Hair_colour", lit("Black painted Blonde")))
    data2Upsert.show
    data2Upsert.createGlobalTempView("changed_data")

    spark.sql(
      """
MERGE INTO harry_ns.input_table base USING global_temp.changed_data incr
    ON base.id = incr.id
WHEN MATCHED THEN
    UPDATE SET base.Hair_colour = incr.Hair_colour
WHEN NOT MATCHED THEN
 INSERT (Id,Name,Gender,Job,House,Wand,Patronus,Species,Blood_status,Hair_colour,Eye_colour,Loyalty,Skills,Birth,Death)
        VALUES(incr.Id,incr.Name,incr.Gender,incr.Job,incr.House,incr.Wand,incr.Patronus,incr.Species,incr.Blood_status,incr.Hair_colour,incr.Eye_colour,incr.Loyalty,incr.Skills,incr.Birth,incr.Death)
      """
    )
    spark.sql("SELECT gender, count(*) FROM harry_ns.input_table GROUP BY gender").show
  }

  //https://iceberg.apache.org/docs/latest/spark-procedures/#remove_orphan_files
  private def optimize(): Unit = {
    val spark: SparkSession = SparkSession
      .builder()
      .master("local[*]")
      .config("spark.driver.host", "localhost")
      // Iceberg configs
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .config("spark.sql.catalog.harry_ns", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.harry_ns.type", "hadoop")
      .config("spark.sql.catalog.harry_ns.warehouse", HARRY_CATALOG_PATH)
      .config("spark.sql.warehouse.dir", WAREHOUSE_DIR_PATH)
      .getOrCreate()
    spark.sql("CALL harry_ns.system.expire_snapshots(table => 'harry_ns.input_table', retain_last => 1)").show
  }
}
